{"transcript": "Today I have the pleasure to speak with John\u00a0 Schulman, who is one of the co-founders of OpenAI\u00a0\u00a0 and leads the post-training team here. He also\u00a0 led the creation of ChatGPT and is the author\u00a0\u00a0 of many of the most important and widely\u00a0 cited papers in AI and RL, including PPO\u00a0\u00a0 and many others. John, really excited to chat\u00a0 with you. Thanks for coming on the podcast.  \u00a0 Thanks for having me on\u00a0 the podcast. I'm a big fan. Thank you for saying that. Here\u2019s my first\u00a0 question. We have these distinctions between\u00a0\u00a0 pre-training and post-training. Let\u2019s\u00a0 go beyond what is actually happening\u00a0\u00a0 in terms of loss function and training\u00a0 regimes. Taking a step back conceptually,\u00a0\u00a0 what kind of thing is pre-training creating?\u00a0 What does post-training do on top of that? \u2028  \u00a0 In pre-training you're basically training to\u00a0 imitate all of the content on the Internet or on\u00a0\u00a0 the web, including websites and code and so forth.\u00a0 So you get a model that can generate content that\u00a0\u00a0 looks like random web pages from the Internet.\u00a0 The model is also trained to maximize likelihood\u00a0\u00a0 where it has to put a probability on everything. \u2028 The objective is basically predicting the next\u00a0\u00a0 token given the previous tokens. Tokens\u00a0 are like words, or parts of words. Since\u00a0\u00a0 the model has to put a probability on it\u2014we're\u00a0 training to maximize log probability\u2014it ends\u00a0\u00a0 up being very calibrated. Not only can it\u00a0 generate all of the content of the web,\u00a0\u00a0 it can also assign probabilities to everything. The base model can effectively take on\u00a0\u00a0 all of these different personas or generate\u00a0 all different kinds of content. When we do\u00a0\u00a0 post-training, we're usually targeting a narrower\u00a0 range of behaviors where we want the model to\u00a0\u00a0 behave like a kind of chat assistant. It's a\u00a0 more specific persona where it's trying to be\u00a0\u00a0 helpful. It's not trying to imitate a person.\u00a0 It's answering your questions or doing your\u00a0\u00a0 tasks. We're optimizing on a different objective,\u00a0 which is more about producing outputs that humans\u00a0\u00a0 will like and find useful, as opposed to just\u00a0 imitating this raw content from the web. \u2028  \u00a0 Maybe I should take a step back and ask this.\u00a0 Right now we have these models that are pretty\u00a0\u00a0 good at acting as chatbots. Taking a step\u00a0 back from how these processes work currently,\u00a0\u00a0 what kinds of things will the models released by\u00a0 the end of the year be capable of doing? What do\u00a0\u00a0 you think the progress will look like if we carry\u00a0 everything forward for the next five years?  \u00a0 The models will get quite a\u00a0 bit better in five years.  \u00a0 In what way?   Even in one or two years,\u00a0\u00a0 we'll find that the models can do a lot more\u00a0 involved tasks than they can do now. For example,\u00a0\u00a0 you could imagine having the models carry out a\u00a0 whole coding project instead of it giving you one\u00a0\u00a0 suggestion on how to write a function. You could\u00a0 imagine the model taking high-level instructions\u00a0\u00a0 on what to code and going out on its own,\u00a0 writing any files, and testing it, and looking\u00a0\u00a0 at the output. It might even iterate on that\u00a0 a bit. So just much more complex tasks. \u2028  \u00a0 Fundamentally the unlock is that it can act\u00a0 coherently for long enough to write multiple files\u00a0\u00a0 of code? What has changed between now and then? \u2028   I would say this will come from some combination\u00a0\u00a0 of training the models to do harder tasks\u00a0 like this. Most of the training data is\u00a0\u00a0 more like doing single steps at a time.\u00a0 I would expect us to do more for training\u00a0\u00a0 the models to carry out these longer projects. \u2028 That\u2019s for any kind of training, like doing RL,\u00a0\u00a0 to learn how to do these tasks. Whether you're\u00a0 supervising the final output or supervising it\u00a0\u00a0 at each step, any kind of training at\u00a0 carrying out these long projects is\u00a0\u00a0 going to make the models a lot better. Since the whole area is pretty new,\u00a0\u00a0 I'd say there's a lot of low-hanging fruit\u00a0 in doing this kind of training. That's one\u00a0\u00a0 thing. I would also expect that as models get\u00a0 better, they get better at recovering from\u00a0\u00a0 errors or dealing with edge cases. When things\u00a0 go wrong, they\u2019ll know how to recover from it. \u2028\u00a0 The models will be more sample efficient. You\u00a0 won't have to collect a ton of data to teach\u00a0\u00a0 them how to get back on track. Just a little\u00a0 bit of data or their generalization from other\u00a0\u00a0 abilities will allow them to get back on track.\u00a0 Current models might just get stuck and get lost.  \u00a0 I want to understand specifically how\u00a0 the generalization helps you get back on\u00a0\u00a0 track. Can you say more about that? I'm not\u00a0 sure why those two concepts are connected.  \u00a0 Right, they're not directly connected. You usually\u00a0 have a little bit of data that does everything.\u00a0\u00a0 If you collect a diverse data set, you're going\u00a0 to get a little bit of everything in it. If you\u00a0\u00a0 have models that generalize really well\u2014even from\u00a0 just a couple of examples of getting back on track\u00a0\u00a0 or if in the pre-training data there are a couple\u00a0 of examples of a model getting back on track\u2014the\u00a0\u00a0 model will be able to generalize from those\u00a0 other things it\u2019s seen to the current situation.\u00a0 If you have models that are weaker, you might\u00a0 be able to get them to do almost anything with\u00a0\u00a0 enough data. But you might have to put a lot\u00a0 of effort into a particular domain or skill. \u2028\u00a0 Whereas for a stronger model,\u00a0 it might just do the right thing\u00a0\u00a0 without any training data or any effort.   Right now these models can work coherently\u00a0\u00a0 for five minutes. We want them to be able to do\u00a0 tasks that a human would take an hour to do, then\u00a0\u00a0 a week, then a month, and so forth.\u2028 To get to each of these benchmarks,\u00a0\u00a0 is it going to be the case that each one takes\u00a0 10X more compute, analogous to the current\u00a0\u00a0 scaling laws for pre-training? Or is it going\u00a0 to be a much more streamlined process of just\u00a0\u00a0 getting to that point where you're already more\u00a0 sample efficient and you can just go straight to\u00a0\u00a0 the years of carrying out tasks or something?\u2028   At a high level, I would agree that longer-horizon\u00a0\u00a0 tasks are going to require more model intelligence\u00a0 to do well. They are going to be more expensive to\u00a0\u00a0 train. I'm not sure I would expect a really\u00a0 clean scaling law unless you set it up in a\u00a0\u00a0 very careful way, or design the experiment in a\u00a0 certain way. There might end up being some phase\u00a0\u00a0 transitions where once you get to a certain\u00a0 level you can deal with much longer tasks.\u00a0 For example, when people do planning for\u00a0 different timescales, I'm not sure they use\u00a0\u00a0 completely different mechanisms. We probably\u00a0 use the same mental machinery thinking about\u00a0\u00a0 one month from now, one year from now, or a\u00a0 hundred years from now. We're not actually\u00a0\u00a0 doing some kind of reinforcement learning\u00a0 where we need to worry about a discount factor\u00a0\u00a0 that covers that timescale and so forth. Using language, you can describe all of\u00a0\u00a0 these different timescales and then you can do\u00a0 things like plan. In the moment you can try to\u00a0\u00a0 make progress towards your goal, whether it's\u00a0 a month away or 10 years away. I don\u2019t know if\u00a0\u00a0 it\u2019s a phase transition but I might expect the\u00a0 same out of models where there might be some\u00a0\u00a0 capabilities that work at multiple scales.   Correct me if this is wrong. It seems like you\u2019re\u00a0\u00a0 implying that right now we have models that are on\u00a0 a per token basis pretty smart. They might be as\u00a0\u00a0 smart as the smartest humans on a per token basis.\u00a0 The thing that prevents them from being as useful\u00a0\u00a0 as they could be is that five minutes from now,\u00a0 they're not going to be still writing your code in\u00a0\u00a0 a way that\u2019s coherent and aligns with your broader\u00a0 goals you have for your project or something.\u00a0 If it's the case that once you start\u00a0 this long-horizon RL training regime\u00a0\u00a0 it immediately unlocks your ability to be\u00a0 coherent for longer periods of time, should\u00a0\u00a0 we be predicting something that is human-level\u00a0 as soon as that regime is unlocked? If not,\u00a0\u00a0 then what is remaining after we can plan for a\u00a0 year and execute projects that take that long?  \u00a0 It's not totally clear what we're going\u00a0 to see once we get into that regime or\u00a0\u00a0 how fast progress will be. That's still uncertain.\u00a0 I wouldn't expect everything to be immediately\u00a0\u00a0 solved by doing any training like this. There'll\u00a0 be other miscellaneous deficits that the models\u00a0\u00a0 have that cause them to get stuck or make worse\u00a0 decisions than humans. I don\u2019t expect that this\u00a0\u00a0 one little thing will unlock all capabilities.\u00a0 But some improvement in the ability to do\u00a0\u00a0 long-horizon tasks might go quite far. \u2028   Would you say it's plausible? Does it seem\u00a0\u00a0 quite likely that there will be other reasons why\u00a0 there might be bottlenecks?\u2028I'm also curious what\u00a0\u00a0 the nature of these bottlenecks might be. It has\u00a0 all these representations of pre-training. Now it\u00a0\u00a0 can work coherently for a long period of time\u00a0 because of long-horizon RL. What's remaining?  \u00a0 Maybe there's some other experience that human\u00a0 experts bring to different tasks such as having\u00a0\u00a0 taste or dealing with ambiguity better. If\u00a0 we want to do something like research I could\u00a0\u00a0 imagine those considerations coming into\u00a0 play. Obviously there are going to be\u00a0\u00a0 mundane limitations around the affordances\u00a0 of the model and whether it can use UIs,\u00a0\u00a0 interact with the physical world, or\u00a0 have access to things. So there might\u00a0\u00a0 be a lot of mundane barriers that\u00a0 are probably not going to last that\u00a0\u00a0 long but would initially slow down progress. \u2028   Let\u2019s talk about the websites that are designed\u00a0\u00a0 for these AIs. Once they\u2019re trained on more\u00a0 multimodal data, will they be in any way different\u00a0\u00a0 from the ones we have for humans? What UIs will be\u00a0 needed? How will it compensate for their strengths\u00a0\u00a0 and weaknesses? How would that look different\u00a0 from the current UIs we have for humans? \u2028  \u00a0 That's an interesting question. I expect that\u00a0 models will be able to use websites that are\u00a0\u00a0 designed for humans just by using vision, after\u00a0 the vision capabilities get a bit better. So there\u00a0\u00a0 wouldn't be an immediate need to change them. On the other hand, there\u2019ll be some websites that\u00a0\u00a0 are going to benefit a lot from AIs being able to\u00a0 use them. We\u2019ll probably want to design those to\u00a0\u00a0 be better UXs for Ais. I'm not sure exactly what\u00a0 that would mean. Assuming that our models are\u00a0\u00a0 still better at text mode than reading text\u00a0 out of images, you'd probably want to have a\u00a0\u00a0 good text-based representation for the models. You\u2019d also want a good indication of what all\u00a0\u00a0 the things that can be interacted with are. But I\u00a0 wouldn't expect the web to get totally redesigned\u00a0\u00a0 to have APIs everywhere. We can get models to\u00a0 use the same kind of UIs that humans use. \u2028  \u00a0 I guess that's been the big lesson of language\u00a0 models, right? That they can act within the\u00a0\u00a0 similar affordances that humans do. I want to go back to the point you\u00a0\u00a0 made earlier about how this process could\u00a0 be more sample efficient because it could\u00a0\u00a0 generalize from its pre-training experiences\u00a0 of how to get unstuck in different scenarios.\u00a0\u00a0 What is the strongest evidence you\u2019ve\u00a0 seen of this generalization and transfer?\u00a0 The big question for the future abilities models\u00a0 seems to be about how much generalization is\u00a0\u00a0 happening. Is there something that feels\u00a0 really compelling to you? Have you seen a\u00a0\u00a0 model learn something that you wouldn't\u00a0 expect it to learn from generalization? \u2028  \u00a0 There have definitely been some interesting\u00a0 instances of generalization in post-training.\u00a0 One well-known phenomenon is that if you\u00a0 do all your fine-tuning with English data,\u00a0\u00a0 the model will automatically behave well\u00a0 in other languages. So if you train the\u00a0\u00a0 assistant on English data, it'll also do something\u00a0 reasonable in Spanish. Sometimes you might get the\u00a0\u00a0 wrong behavior in terms of whether it replies\u00a0 in English or replies in Spanish. Usually you\u00a0\u00a0 get the right behavior there, meaning you get it\u00a0 to respond in Spanish to Spanish queries. That's\u00a0\u00a0 one interesting instance of generalization\u00a0 where the model just latches onto the right,\u00a0\u00a0 helpful persona and then automatically does\u00a0 the right thing in different languages. \u2028\u00a0 We've seen some version of this with multimodal\u00a0 data where if you do text-only fine-tuning,\u00a0\u00a0 you also get reasonable behavior with images.\u00a0 Early on in ChatGPT we were trying to fix some\u00a0\u00a0 issues with the model understanding its own\u00a0 limitations. Early versions of the model would\u00a0\u00a0 think that it could send you an email or call\u00a0 you an Uber or something. The model would try\u00a0\u00a0 to play the assistant and it would say \u201coh yeah,\u00a0 of course I sent that email.\u201d Obviously it didn't.\u00a0 So we started collecting some data to fix\u00a0 those problems. We found that a tiny amount\u00a0\u00a0 of data did the trick, even when you mixed\u00a0 it together with everything else. I don't\u00a0\u00a0 remember exactly how many examples but something\u00a0 like 30 examples. We had a pretty small number\u00a0\u00a0 of examples showing this general behavior,\u00a0 explaining that the model doesn\u2019t have this\u00a0\u00a0 capability. That generalized pretty well to all\u00a0 sorts of capabilities we didn't train for. \u2028  \u00a0 I still want to go back to this because I'm not\u00a0 sure I understood. Let\u2019s say you have this model\u00a0\u00a0 that is trained to be coherent for longer\u00a0 periods of time. Setting aside these other\u00a0\u00a0 bottlenecks which there may or may not be, by\u00a0 next could you have models that are potentially\u00a0\u00a0 like human-level? I\u2019m thinking of a model that\u00a0 you\u2019re interacting with like a colleague and it's\u00a0\u00a0 as good as interacting with a human colleague.\u00a0 You can tell them to go do stuff and they go get\u00a0\u00a0 it done. What seems wrong with that picture of\u00a0 the capabilities you think might be possible?  \u00a0 It's hard to say exactly what the deficit\u00a0 will be. When you talk to the models today,\u00a0\u00a0 they have various weaknesses besides long-term\u00a0 coherence. They also struggle to really think\u00a0\u00a0 hard about things or pay attention to what\u00a0 you ask them. I wouldn't expect improving\u00a0\u00a0 the coherence a little bit to be all it takes to\u00a0 get to AGI. I guess I can\u2019t articulate exactly\u00a0\u00a0 what are the main weaknesses that will stop\u00a0 them from being a fully functional colleague.    \u00a0 It seems like then, you should be planning for\u00a0 the possibility you would have AGI very soon.  \u00a0 I think that would be reasonable.    So what's the plan if there's no\u00a0\u00a0 other bottlenecks. In the next year or\u00a0 something, you\u2019ve got AGI. What's the plan?  \u00a0 If AGI came way sooner than expected we\u00a0 would definitely want to be careful about\u00a0\u00a0 it. We might want to slow down a little bit\u00a0 on training and deployment until we're pretty\u00a0\u00a0 sure we know we can deal with it safely. We\u00a0 would need a pretty good handle on what it's\u00a0\u00a0 going to do and what it can do. We would\u00a0 have to be very careful if it happened way\u00a0\u00a0 sooner than expected. Our understanding\u00a0 is still rudimentary in a lot of ways.  \u00a0 What would being careful mean? Presumably\u00a0 you're already careful, right? You do these\u00a0\u00a0 evaluations before deploying.    Maybe it means not training the\u00a0\u00a0 even smarter version or being really careful\u00a0 when you do train it. You can make sure it\u2019s\u00a0\u00a0 properly sandboxed and everything. Maybe\u00a0 it means not deploying it at scale or\u00a0\u00a0 being careful about what scale you deploy it at.   Let's just play with a scenario. AGI happens next\u00a0\u00a0 year. You're not training a smarter system but\u00a0 you're deploying it in a somewhat measured way.\u00a0\u00a0 Presumably the development wouldn\u2019t be particular\u00a0 to OpenAI. AGI just turns out to be much easier\u00a0\u00a0 than we expected and that\u2019s why it happened.\u00a0 So you wait to deploy a little bit. Now other\u00a0\u00a0 companies have a similar level of capabilities.\u00a0 What happens next? While you wait to deploy,\u00a0\u00a0 what are you waiting for? What is every\u00a0 company doing in this scenario? \u2028  \u00a0 The game theory is a little tough\u00a0 to think through. First of all,\u00a0\u00a0 I don't think this is going to\u00a0 happen next year but it's still\u00a0\u00a0 useful to have the conversation. It\u00a0 could be two or three years instead.  \u00a0 Two or three years is still pretty soon.   It\u2019s still pretty soon. You probably need\u00a0\u00a0 some coordination. Everyone needs to agree\u00a0 on some reasonable limits to deployment or\u00a0\u00a0 to further training for this to work.\u00a0 Otherwise you have the race dynamics\u00a0\u00a0 where everyone's always trying to stay ahead and\u00a0 that might require compromising safety. You would\u00a0\u00a0 probably need some coordination among the larger\u00a0 entities that are doing this kind of training.  \u00a0 You'd be coordinating to pause\u00a0 deployment until what exactly? Until\u00a0\u00a0 you figure out what's happening in the model?   We could pause further training. We could pause\u00a0\u00a0 deployment. We could avoid certain types\u00a0 of training that might be riskier. We\u00a0\u00a0 would set up some reasonable rules for what\u00a0 everyone should do to limit these things.  \u00a0 Limit to what end? At some point the potential\u00a0 energy that's within this intelligence will\u00a0\u00a0 be unleashed. Suppose in two years we get\u00a0 the AGI. Now everybody's freaking out. The\u00a0\u00a0 AI companies have paused. What would\u00a0 be the thing we plan to wait until?  \u00a0 I don't have a good answer to that. If we\u00a0 can coordinate like that, that would be a\u00a0\u00a0 pretty good scenario. Building these models\u00a0 is very capital intensive and there are a lot\u00a0\u00a0 of complex pieces. It's not like everyone's\u00a0 going to go and recreate this stuff at home.\u00a0 Given the relatively small number of\u00a0 entities who could train the largest models,\u00a0\u00a0 it does seem possible to coordinate. I'm not\u00a0 sure how you would maintain this equilibrium for\u00a0\u00a0 a long period of time, but I think if we got\u00a0 to that point we would be in an okay position.  \u00a0 Would we? I'm still curious because I'm\u00a0 not sure what happens next. Fundamentally,\u00a0\u00a0 the benefit is that you push it to the server\u00a0 and now we have a bunch of intelligences,\u00a0\u00a0 or they could push themselves to the server.\u00a0 Now we\u2019ve got everybody coordinated but I'm\u00a0\u00a0 not sure what we do next in this world. Why\u00a0 does that set us up for a good outcome?  \u00a0 If we had everyone reasonably coordinated and we\u00a0 felt like we could solve the technical problems\u00a0\u00a0 around alignment well enough then we could deploy.\u00a0 We would be able to deploy really smart AIs that\u00a0\u00a0 can act as extensions of people's wills but also\u00a0 prevent them from being catastrophically misused.\u00a0\u00a0 That would be great. We could go ahead and\u00a0 safely deploy these systems and it would usher\u00a0\u00a0 in a lot of prosperity and a much more rapid\u00a0 phase of scientific advancement. That would\u00a0\u00a0 be what the good scenario would look like. \u2028   That makes sense. I\u2019m curious about something\u00a0\u00a0 down the road in a couple of years. In the\u00a0 best case scenario, all these actors have\u00a0\u00a0 agreed to pause until we've figured out that we're\u00a0 building aligned systems that are not themselves\u00a0\u00a0 going to attempt a coup or not going to enable\u00a0 somebody else to do that. What would proof of that\u00a0\u00a0 look like? What would evidence of that look like?   If we can deploy systems that are incrementally\u00a0\u00a0 that are successively smarter than the\u00a0 ones before, that would be safer. I hope\u00a0\u00a0 the way things play out is not a scenario where\u00a0 everyone has to coordinate, lock things down,\u00a0\u00a0 and safely release things. That would lead\u00a0 to this big buildup in potential energy.\u00a0 I would rather have a scenario where we're all\u00a0 continually releasing things that are a little\u00a0\u00a0 better than what came before. We\u2019d be doing\u00a0 this while making sure we\u2019re confident that\u00a0\u00a0 each diff improves on safety and alignment in\u00a0 correspondence to the improvement in capability.\u00a0\u00a0 If things started to look a little bit\u00a0 scary, then we would be able to slow\u00a0\u00a0 things down. That's what I would hope for. If there's more of a discontinuous jump,\u00a0\u00a0 there\u2019s a question of \u201chow do you know if the\u00a0 thing you've got is safe to release\u201d. I can't\u00a0\u00a0 give a generic answer. However, the type of thing\u00a0 you might want to do to make that more acceptable\u00a0\u00a0 would be a lot of testing simulated deployment,\u00a0 red teaming of sorts. You'd want to do that in a\u00a0\u00a0 way that is much more likely to fail than the\u00a0 thing you\u2019re planning to do in the real world.\u00a0 You'd want to have a really good monitoring system\u00a0 so that if something does start to go wrong with\u00a0\u00a0 the deployed system, you can immediately detect\u00a0 it. Maybe you've got something watching over\u00a0\u00a0 the deployed AIs, watching what they're\u00a0 doing, and looking for signs of trouble. \u2028\u00a0 You\u2019d want some defense in depth. You'd want\u00a0 some combination of \u201cthe model itself seems to be\u00a0\u00a0 really well-behaved with impeccable, moral\u00a0 confidence in everything\u201d and \u201cI\u2019m pretty\u00a0\u00a0 confident that it\u2019s extremely resistant to any\u00a0 kind of severe misuse.\u201d You'd also want really\u00a0\u00a0 good monitoring on top of it so you could\u00a0 detect any kind of unforeseen trouble. \u2028  \u00a0 What are you keeping track of while\u00a0 you're doing long-horizon RL or when\u00a0\u00a0 you eventually start doing it? How could\u00a0 you notice this sort of discontinuous jump\u00a0\u00a0 before you deployed these systems broadly?   You would want to have a lot of evals that\u00a0\u00a0 you're running during the training process.\u2028   What specifically? Does it make sense to train on\u00a0\u00a0 a long-horizon RL knowing that this is something\u00a0 that could happen? Or is it just a very low\u00a0\u00a0 possibility? How do you think about this? \u2028   You'd want to be pretty careful when you\u00a0\u00a0 do this kind of training if you see a\u00a0 lot of potentially scary capabilities.\u00a0\u00a0 I would say it's not something we have to be\u00a0 scared of right now because right now it's\u00a0\u00a0 hard to get the models to do anything coherent. If they started to get really good, we would\u00a0\u00a0 want to take some of these questions seriously.\u00a0 We would want to have a lot of evals that test\u00a0\u00a0 them for misbehavior, mostly for the alignment\u00a0 of the models. We'd want to check that they\u2019re\u00a0\u00a0 not going to turn against us or something. You\u00a0 might also want to look for discontinuous jumps\u00a0\u00a0 in capabilities. You'd want to have lots of\u00a0 evals for the capabilities of the models.\u00a0 You'd also want to make sure that whatever\u00a0 you're training on doesn't have any reason to\u00a0\u00a0 make the model turn against you. That doesn't\u00a0 seem like the hardest thing to do. The way we\u00a0\u00a0 train them with RLHF, that does feel very\u00a0 safe even though the models are very smart.\u00a0\u00a0 The model is just trying to produce a message\u00a0 that is pleasing to a human. It has no concern\u00a0\u00a0 about anything else in the world other than\u00a0 whether the text it produces is approved. \u2028\u00a0 Obviously if you were doing something where\u00a0 the model has to carry out a long sequence\u00a0\u00a0 of actions which involve tools, then it\u00a0 might have some incentive to do a lot of\u00a0\u00a0 wacky things that wouldn't make sense to a\u00a0 human in the process of producing its final\u00a0\u00a0 result. However, it wouldn't necessarily have\u00a0 an incentive to do anything other than produce\u00a0\u00a0 a very high quality output at the end. There are old points about instrumental\u00a0\u00a0 convergence where the model wants to take over\u00a0 the world so it can produce some awesome piece\u00a0\u00a0 of code at the end. If you ask it to write\u00a0 you a Flask app, it'll be like \u201coh yeah,\u00a0\u00a0 first I need to take over the world. At a\u00a0 certain point it's a little hard to imagine\u00a0\u00a0 why for fairly well specified tasks like coding\u00a0 an app, you would want to first take over the\u00a0\u00a0 world. Of course if you assigned a task such as\u00a0 \u201cmake money,\u201d then maybe that would lead to some\u00a0\u00a0 nefarious behavior as an instrumental goal.   Before we get back to that, let's step back\u00a0\u00a0 and talk about today\u2019s RLHF systems and\u00a0 everything. I do want to follow up on that\u00a0\u00a0 point because it is interesting. With today's RLHF and the way in which\u00a0\u00a0 it influences these models, how would\u00a0 you characterize it in terms of human\u00a0\u00a0 psychology? Is it a drive? Is it a goal? Is\u00a0 it an impulse? Psychologically, what kind of\u00a0\u00a0 thing is it? In what way is it being changed? Not simply the persona of a chatbot but \u201cdon't\u00a0\u00a0 talk that way, talk this other way\u201d or\u00a0 \u201cdon\u2019t put out those kinds of outputs.\u201d  \u00a0 There are probably some analogies\u00a0 with a drive or a goal in humans.\u00a0\u00a0 You're trying to steer towards a certain\u00a0 set of states rather than some other\u00a0\u00a0 states. I would think that our concept\u00a0 of a drive or a goal has other elements,\u00a0\u00a0 such as the feeling of satisfaction you get for\u00a0 achieving it. Those things have more to do with\u00a0\u00a0 the learning algorithm than what the model does\u00a0 at runtime when you just have a fixed model.\u00a0 There are probably some analogies though I don't\u00a0 know exactly how close it is. To some extent,\u00a0\u00a0 the models do have drives and goals in\u00a0 some meaningful way. In the case of RLHF\u00a0\u00a0 where you're trying to maximize human\u00a0 approval as measured by a reward model,\u00a0\u00a0 the model is just trying to produce something that\u00a0 people are going to like and judge as correct.  \u00a0 I\u2019ve heard two ideas in terms of using that\u00a0 internal monologue to get better at reasoning.\u00a0\u00a0 At least publicly, I've seen two ideas and I'm\u00a0 curious which one you think is more promising.\u00a0 One is that the model learns from its outputs\u00a0 over a bunch of potential trains of thought,\u00a0\u00a0 and it learns to follow the one that leads\u00a0 to the correct answer. It is then trained\u00a0\u00a0 on that before deployment. The other one is\u00a0 you use a bunch of compute to do inference\u00a0\u00a0 in deployment. This approach involves the\u00a0 model talking to itself while it's deployed.\u00a0 Which one do you expect to be closer to the way\u00a0 a model has been trained when it gets really good\u00a0\u00a0 at reasoning? Is it because it's doing just a\u00a0 bunch of inference clouds? Is it just because\u00a0\u00a0 you've trained it to do well at that? \u2028   You could define reasoning as tasks that\u00a0\u00a0 require some kind of computation at test time\u00a0 or maybe some kind of deduction. By definition,\u00a0\u00a0 reasoning would be tasks that require some test\u00a0 time computation and step-by-step computation.\u00a0\u00a0 On the other hand, I would also expect to\u00a0 gain a lot from doing practice at training\u00a0\u00a0 time. So I think that you\u2019d get the best\u00a0 results by combining these two things. \u2028  \u00a0 Right now, you have these two ways the\u00a0 model learns. One is in training, whether\u00a0\u00a0 it's pre-training or post-training. Most of the\u00a0 compute in training is spent on pre-training,\u00a0\u00a0 glossing over trillions of tokens, skimming\u00a0 trillions of tokens worth of information.\u00a0\u00a0 If a human was subjected to that, they\u00a0 would just be totally confused. It's\u00a0\u00a0 just not a very efficient way to learn. The other way is in-context learning.\u00a0\u00a0 Of course that is more sample-efficient,\u00a0 but it's destroyed with each instance.\u00a0 I'm curious if you think that there's\u00a0 a path for something in between those,\u00a0\u00a0 where it\u2019s not destroyed at each instance but it's\u00a0 also not as frivolous as just seeing trillions of\u00a0\u00a0 tokens. Something more deliberate and active. \u2028   Do you mean models having some kind of medium-term\u00a0\u00a0 memory? Too much to fit in context but\u00a0 much smaller scale than pre-training?  \u00a0 It might be memory. I don't have context.\u00a0 Certainly when I'm trying to prepare for\u00a0\u00a0 this conversation, I think of what I\u00a0 should understand, read it carefully,\u00a0\u00a0 and maybe think about it as I\u2019m reading it. I\u2019m\u00a0 not sure what it naturally corresponds to in\u00a0\u00a0 terms of models. What would that look like?   I see. So it\u2019s not just memory but it\u2019s also\u00a0\u00a0 somewhat specializing to a certain task or putting\u00a0 a lot of effort into some particular project.  \u00a0 I'm not even sure if it's specialization.\u00a0 It\u2019s more so \u201cI don't understand this part,\u00a0\u00a0 so let me look into it more deeply. I\u00a0 already understand this.\u201d I guess it\u2019s\u00a0\u00a0 specializing to your existing knowledge base.   I see. So it's not just about training on a bunch\u00a0\u00a0 of sources that are relevant and fine-tuning\u00a0 on some special domain. It's also about\u00a0\u00a0 reasoning and developing some knowledge\u00a0 through your own reasoning and using some\u00a0\u00a0 sort of introspection or self-knowledge\u00a0 to figure out what it needs to learn?  \u00a0 Yeah.   That does feel like something that's missing\u00a0\u00a0 from today's systems. People haven't really pushed\u00a0 too hard on this middle ground between large-scale\u00a0\u00a0 training\u2014where you produce a single snapshot model\u00a0 that's supposed to do everything like a deployed\u00a0\u00a0 model\u2014and on the other hand in-context learning. Part of that is that we've just been increasing\u00a0\u00a0 context length so much that there hasn't\u00a0 been an incentive for it. If you can go\u00a0\u00a0 to a hundred thousand or a million context,\u00a0 then that's actually quite a lot. It\u2019s not\u00a0\u00a0 actually the bottleneck in a lot of cases. I agree that you'd probably also want to\u00a0\u00a0 supplement that with some kind of fine-tuning.\u00a0 The capabilities you get from fine-tuning and\u00a0\u00a0 in-context learning are probably somewhat\u00a0 complementary. I\u2019d expect us to want to build\u00a0\u00a0 systems that do some online learning and also\u00a0 have some cognitive skills, like introspecting\u00a0\u00a0 on their own knowledge and seeking out\u00a0 new knowledge that fills in the holes.  \u00a0 Is this all happening at the same time?\u00a0 Is it just a new training regime where\u00a0\u00a0 all these things can happen at once, whether\u00a0 it\u2019s long-horizon or this kind of training?\u00a0 Are they separate or not? Is the model smart\u00a0 enough to both introspect and act on longer\u00a0\u00a0 horizons so that you get adequate\u00a0 reward on the long-horizon tasks?  \u00a0 If you're doing some kind of long-horizon task,\u00a0 you're learning while you do the task, right? \u2028\u00a0 The only way to do something that involves a lot\u00a0 of steps is to have learning and memory that gets\u00a0\u00a0 updated during the task. There\u2019s a continuum\u00a0 between short-term and long-term memory.\u00a0 I expect the need for this capability would start\u00a0 to become clear when we start to look more at\u00a0\u00a0 long-horizon tasks. To some extent, putting a\u00a0 lot of stuff into context will take you pretty\u00a0\u00a0 far because we have really long context now.\u00a0 You probably also want things like fine-tuning.\u00a0 As for introspection and the ability to do active\u00a0 learning, that might automatically fall out of\u00a0\u00a0 the models\u2019 abilities to know what they know.\u00a0 Models do have some calibration regarding what\u00a0\u00a0 they know. That's why models don't hallucinate\u00a0 that badly. They have some understanding of\u00a0\u00a0 their own limitations. That same kind of ability\u00a0 could be used for something like active learning. There are all these complicated RL procedures,\u00a0 many of which you've pioneered. How many of them\u00a0\u00a0 will be relevant when you get to the point where\u00a0 the model itself is smart enough to act as its\u00a0\u00a0 own environment and interact in a more online\u00a0 and stable way? Is the path for progress going\u00a0\u00a0 to be more straightforward than the kinds of\u00a0 solutions that were required for RL in the past? I think policy grading algorithms are not\u00a0 the most sample efficient algorithms. So\u00a0\u00a0 that's probably not what you want to do\u00a0 at test time if you want to learn really\u00a0\u00a0 fast. But who knows? Maybe it's not that bad.\u00a0 I think motor learning in animals is probably\u00a0\u00a0 something like a policy grading algorithm. \u2028 For example, let\u2019s say you're learning how\u00a0\u00a0 to shoot baskets. That probably takes\u00a0 thousands of tries to get more accurate.\u00a0\u00a0 There's probably something like a policy grading\u00a0 algorithm underneath. That\u2019s not going to be\u00a0\u00a0 the fastest way to learn if you have a model\u00a0 trying to do a project or some kind of task.\u00a0 We would want to rely more on in-context learning\u00a0 where you effectively have a learned algorithm.\u00a0\u00a0 It\u2019s learned how to explore. It\u2019s learned\u00a0 how to try all the possibilities exhaustively\u00a0\u00a0 instead of doing the same thing over and\u00a0 over again and making the same mistake.\u00a0\u00a0 We'll be able to do things that look more like\u00a0 learned search algorithms. That'll be the kind\u00a0\u00a0 of thing that gets used in a particular task.     Interesting. I want to step back and ask about\u00a0\u00a0 your own history, at least at OpenAI. You\u00a0 led the creation of ChatGPT.\u2028At what point\u00a0\u00a0 did you realize that these LLMs are the path to\u00a0 go? When did you realize a chatbot or some way\u00a0\u00a0 to instruct them would be useful? Just walk me\u00a0 through the whole lineage from when this became\u00a0\u00a0 your main focus and what the process was like.   Before ChatGPT, OpenAI had these instruction\u00a0\u00a0 following models. The idea there was that we\u00a0 had base models that people could prompt them\u00a0\u00a0 in elaborate ways. But they were also hard to\u00a0 prompt. They basically do autocomplete so you had\u00a0\u00a0 to set up a very good prompt with some examples. People at OpenAI were working on just taking the\u00a0\u00a0 base models and making them easier to prompt. So\u00a0 if you just wrote a question it would answer the\u00a0\u00a0 question, instead of giving you more questions or\u00a0 something. So we had these instruction following\u00a0\u00a0 models, which were like base models but a\u00a0 little easier to use. Those are the original\u00a0\u00a0 ones deployed in the API. Or after GPT-3,\u00a0 those were the next generation of models.\u00a0 At the same time there were definitely a lot\u00a0 of people thinking about chat. Google had some\u00a0\u00a0 papers like LaMDA and earlier, Meena. They\u00a0 had these chatbots. It was more like a base\u00a0\u00a0 model that was really specialized to the\u00a0 task of chat. It was really good at chat.\u00a0\u00a0 Looking at the examples from the paper, it\u00a0 was more used for fun applications where the\u00a0\u00a0 model would take on some persona and pretend\u00a0 to be that persona. It was not so functional\u00a0\u00a0 where it could help me refactor my code. So there were definitely people thinking\u00a0\u00a0 about chat. I had worked before on a\u00a0 project looking at chat called WebGPT,\u00a0\u00a0 which was more about doing question answering with\u00a0 the help of web browsing and retrieval. When you\u00a0\u00a0 do question answering, it really wants to be in a\u00a0 chat. You always want to ask follow-up questions\u00a0\u00a0 or sometimes the model should ask a clarifying\u00a0 question because the question is ambiguous. \u2028\u00a0 It was clear after we did the first version, that\u00a0 the next version should be conversational. So we\u00a0\u00a0 started working on the conversational chat\u00a0 assistant. This was built on top of GPT-3.5,\u00a0\u00a0 which was done training at the beginning of\u00a0 2022. That model was quite good at language\u00a0\u00a0 and code.\u2028 We quickly realized that it was\u00a0 actually quite good at coding help. That was\u00a0\u00a0 one of the things we were excited about. We worked on that for most of the year.\u00a0\u00a0 We had browsing as another feature in\u00a0 it although we ended up deemphasizing\u00a0\u00a0 that later on because the model's internal\u00a0 knowledge was so good. The browsing wasn't\u00a0\u00a0 the most interesting thing about it. We had it\u00a0 out to friends and family for a while and we\u00a0\u00a0 were thinking about doing a public release. Actually, GPT-4 finished training in August\u00a0\u00a0 that year. The flagship RL effort at OpenAI was\u00a0 the instruction following effort because those\u00a0\u00a0 were the models that were being deployed into\u00a0 production. The first fine-tunes of GPT-4 used\u00a0\u00a0 that whole stack. Those models were really\u00a0 good and everyone got really excited about\u00a0\u00a0 that after seeing the instruct fine tune GPT-4s. They were really good. They would occasionally\u00a0\u00a0 give you amazing outputs, but the model\u00a0 was clearly also pretty unreliable. It\u00a0\u00a0 would sometimes hallucinate it a lot. It\u00a0 would sometimes give you pretty unhinged\u00a0\u00a0 outputs. So it was clearly not quite ready for\u00a0 prime time, but it was obviously very good.\u00a0 People forgot about chat for a little while\u00a0 after that, this alternative branch. We pushed\u00a0\u00a0 it further and we ended up mixing together all the\u00a0 datasets, the instruct and the chat data, to try\u00a0\u00a0 to get something that was the best of both worlds.\u00a0 The chat models were clearly easier to use.\u00a0 It automatically had much more sensible behavior\u00a0 in terms of the model knowing its own limitations.\u00a0\u00a0 That was actually one of the things that I\u00a0 got excited about as we were developing it.\u00a0\u00a0 I realized a lot of the things that people\u00a0 thought were flaws in language models, like\u00a0\u00a0 blatant hallucination, could be not completely\u00a0 fixed but things that you could make a lot of\u00a0\u00a0 progress on with pretty straightforward methods. The other thing about chat was when we had these\u00a0\u00a0 instruct models. The task of \u201ccomplete this text,\u00a0 but in a nice or helpful way\u201d is a pretty poorly\u00a0\u00a0 defined task. That task is both confusing for the\u00a0 model and for the human who's supposed to do the\u00a0\u00a0 data labeling. Whereas for chat, people had an\u00a0 intuitive sense of what a helpful robot should\u00a0\u00a0 be like. So it was just much easier for people to\u00a0 get an idea of what the model was supposed to do.\u00a0\u00a0 As a result, the model had a much more\u00a0 coherent personality and it was much easier\u00a0\u00a0 to get pretty sensible behavior robustly.   Interesting. Is it the case that anybody\u00a0\u00a0 could have made ChatGPT using your\u00a0 publicly available fine-tuning API? \u2028  \u00a0 Not exactly. I don't remember which models\u00a0 were available for fine-tuning. Assuming we\u00a0\u00a0 had 3.5 available for fine-tuning at the time, you\u00a0 could have made something decently close. I don't\u00a0\u00a0 think you would have been able to do just one\u00a0 iteration of fine-tuning with purely human-written\u00a0\u00a0 data. You'd want to do several iterations. If not you're not going to do RL, which we did,\u00a0\u00a0 you\u2019d want some kind of iterative supervised\u00a0 fine-tuning where humans edit the model-generated\u00a0\u00a0 outputs. If you train on human-generated\u00a0 data, even if it\u2019s really high quality,\u00a0\u00a0 it\u2019s just hard for a model to fit that data\u00a0 perfectly because it might be something a model\u00a0\u00a0 is capable of outputting. You need to do something\u00a0 iterative that looks a bit more like RL. If you\u2019d\u00a0\u00a0 done that, you could have gotten pretty\u00a0 close but it would have been non-trivial.\u00a0 We also had another instruction-following\u00a0 model trained with RL, released a little\u00a0\u00a0 before ChatGPT. If you put a chat wrapper\u00a0 on that you would\u2019ve gotten decently close\u00a0\u00a0 but that model had some differences in\u00a0 strengths. That model was good at writing\u00a0\u00a0 and poetry but it wasn\u2019t as good at knowing\u00a0 its limitations, factuality, and so forth.  \u00a0 Stepping back from 3.5, I think I heard\u00a0 you say somewhere that you were super\u00a0\u00a0 impressed with GPT-2. Compared to your\u00a0 expectations in 2019, has AI progressed\u00a0\u00a0 faster or slower than you would have expected?   Faster than I expected since GPT-2. I was pretty\u00a0\u00a0 bought into scaling and pre-training being a good\u00a0 idea. But when GPT-2 was done, I wasn't completely\u00a0\u00a0 sold on it being revolutionizing everything. It\u00a0 was really after GPT-3 that I pivoted what I was\u00a0\u00a0 working on and what my team was working on. After\u00a0 that, we got together and said, \"oh yeah, let's\u00a0\u00a0 see what we can do here with this language model\u00a0 stuff.\" But after GPT-2, I wasn't quite sure yet.  \u00a0 Let\u2019s say the stuff we were talking about\u00a0 earlier with RL starts working better with\u00a0\u00a0 these smarter models. Does the fraction\u00a0 of compute that is spent on pre-training\u00a0\u00a0 versus post-training change significantly\u00a0 in favor of post-training in the future?  \u00a0 There are some arguments for that.\u00a0 Right now it's a pretty lopsided ratio.\u00a0\u00a0 You could argue that the output generated\u00a0 by the model is higher quality than most of\u00a0\u00a0 what's on the web. So it makes more sense for\u00a0 the model to think by itself rather than just\u00a0\u00a0 training to imitate what's on the web. So I think\u00a0 there's a first principles argument for that.\u00a0 We found a lot of gains through post-training.\u00a0 So I would expect us to keep pushing this\u00a0\u00a0 methodology and probably increasing\u00a0 the amount of compute we put into it.  \u00a0 The current GPT-4 has an Elo score that is like\u00a0 a hundred points higher than the original one\u00a0\u00a0 that was released. Is that all because of what\u00a0 you're talking about, with these improvements\u00a0\u00a0 that are brought on by post-training? \u2028   Yeah, most of that is post-training.\u00a0\u00a0 There are a lot of different,\u00a0 separate axes for improvement. \u2028\u00a0 We think about data quality, data quantity.\u00a0 There\u2019s just doing more iterations of the\u00a0\u00a0 whole process of deploying and collecting\u00a0 new data. There\u2019s also changing what kind\u00a0\u00a0 of annotations you're collecting. There's a lot\u00a0 of things that stack up but together they give\u00a0\u00a0 you a pretty good effective compute increase.   That's a huge increase. It's really interesting\u00a0\u00a0 that there's this much room for\u00a0 improvement from post-training.\u00a0 What makes for somebody who's really good at\u00a0 doing this sort of RL research? I hear it's super\u00a0\u00a0 finicky. What is the sort of intuition that you\u00a0 have that enables you to find these ways to mess\u00a0\u00a0 with the data and set up these environments?   I have a decent amount of experience at this\u00a0\u00a0 point from the different parts of the stack,\u00a0 from RL algorithms, which I've worked on since\u00a0\u00a0 grad school, to data collection, annotation\u00a0 processes, and playing with language models.\u00a0 I'd say I've dabbled with these things and the\u00a0 people who do well at this kind of research have\u00a0\u00a0 some view of the whole stack and have a lot\u00a0 of curiosity about the different parts of it.\u00a0\u00a0 You want to be both empirical and let experiments\u00a0 update your views, but you also want to think from\u00a0\u00a0 first principles. Assuming that learning works,\u00a0 what would be the ideal type of data to collect?\u00a0\u00a0 That type of thing.   Because there doesn't\u00a0\u00a0 seem to be a model since GPT-4 that seems to be\u00a0 significantly better, there's a hypothesis that\u00a0\u00a0 we might be hitting some sort of plateau. These\u00a0 models aren't actually generalizing that well,\u00a0\u00a0 and you're going to hit a data wall beyond\u00a0 which the abilities unlocked by memorizing a\u00a0\u00a0 vast corpus of pre-training data won't help\u00a0 you get something much smarter than GPT-4.\u00a0 Do you think that hypothesis is wrong? We've\u00a0 talked about some examples of generalization,\u00a0\u00a0 like Spanish to English. One example I think of is\u00a0 the transfer from code to reasoning in language.\u00a0\u00a0 If you train on a bunch of code, it gets better at\u00a0 reasoning in language? Is that actually the case?\u00a0 Do you see positive transfer between different\u00a0 modalities? If you train on a bunch of videos\u00a0\u00a0 and images, it'll get smarter from synthetic data?\u00a0 Or does it seem like the abilities unlocked are\u00a0\u00a0 extremely local to the exact kind of labels\u00a0 and data you put into the training corpus?  \u00a0 I'll try to respond to all that.\u00a0 First, are we about to hit the\u00a0\u00a0 data wall? I wouldn't draw too much from the\u00a0 time since GPT-4 was released because it does\u00a0\u00a0 take a while to train these models and do all\u00a0 the prep to train a new generation of models.\u00a0 I wouldn't draw too much from that fact. There\u00a0 are definitely some challenges from the limited\u00a0\u00a0 amount of data, but I wouldn't expect us\u00a0 to immediately hit the data wall. However,\u00a0\u00a0 I would expect the nature of pre-training to\u00a0 somewhat change over time as we get closer to it.\u00a0 In terms of generalization from different types\u00a0 of pre-training data, I would say it's pretty hard\u00a0\u00a0 to do science on this type of question because\u00a0 you can't create that many pre-trained models.\u00a0\u00a0 Maybe you can't train a GPT-4 sized model and\u00a0 do ablation studies at that scale. Maybe you\u00a0\u00a0 can train a ton of GPT-2 size models or even\u00a0 a GPT-3 size model with different data blends\u00a0\u00a0 and see what you get. I'm not aware of any\u00a0 public results on ablations involving code\u00a0\u00a0 data and reasoning performance and so forth. I'd\u00a0 be very interested to know about those results.  \u00a0 I'm curious about something. One of the things\u00a0 is that the model gets smarter as it gets\u00a0\u00a0 bigger. Would an ablation on a GPT-2 level model,\u00a0 which suggests that there isn't much transfer,\u00a0\u00a0 provide evidence for the level of transfer on a\u00a0 similar set of domains in a GPT-4 level model?  \u00a0 Right, you might not be able to conclude that\u00a0 if transfer fails at GPT-2 size, then it's\u00a0\u00a0 also going to fail at a higher scale. It might\u00a0 be that for the larger models, you learn better\u00a0\u00a0 shared representations, whereas the smaller models\u00a0 have to lean too much on memorization. The larger\u00a0\u00a0 models can learn how to do the right computation.\u00a0 I would expect this to be true to some extent.  \u00a0 This might have a very simple answer. You\u00a0 train bigger models on the same amount of data\u00a0\u00a0 and they become smarter. Or to get the same\u00a0 level of intelligence, you only have to train\u00a0\u00a0 them on less data. Why is that the case?\u00a0 It's got more parameters, seen fewer things,\u00a0\u00a0 and now it's equally as smart. Why is that?   I don't think anyone has a good explanation\u00a0\u00a0 for the scaling law with parameter count.\u00a0 I don't even know what the best mental\u00a0\u00a0 model is for this. Clearly, you have more\u00a0 capacity if you have a bigger model. So you\u00a0\u00a0 should eventually be able to get lower loss. Why are bigger models more sample efficient?\u00a0\u00a0 I can give you a sketchy explanation. You could\u00a0 say that the model is an ensemble of different\u00a0\u00a0 circuits that do the computation. You could\u00a0 imagine that it's doing computations in parallel\u00a0\u00a0 and the output is a weighted combination of them.\u00a0 If you have more width\u2026 actually width is somewhat\u00a0\u00a0 similar to depth because with residual networks,\u00a0 depth can do something similar to width in\u00a0\u00a0 terms of updating what's in the residual stream. You're learning all these different computations\u00a0\u00a0 in parallel and you have more of them with a\u00a0 bigger model. So you have a higher chance that\u00a0\u00a0 one of them is lucky, ends up guessing\u00a0 correctly a lot, and gets upweighted.\u00a0 There are some algorithms that work this way,\u00a0 like mixture models or multiplicative weight\u00a0\u00a0 update algorithms, where you have\u2014I don\u2019t\u00a0 want to say mixture of experts because it\u00a0\u00a0 means something different\u2014basically a weighted\u00a0 combination of experts with some learned gating.\u00a0 I actually said something slightly wrong,\u00a0 but you could imagine something like that.\u00a0\u00a0 Just having a bigger model gives you\u00a0 more chances to get the right function.\u00a0 Of course, it's not just totally disjoint\u00a0 functions you're taking a linear combination\u00a0\u00a0 of. It's more like a library where you might\u00a0 chain the functions together in some way.\u00a0\u00a0 There's some composability. So I would say a\u00a0 bigger model has a bigger library of different\u00a0\u00a0 computations, including lots of stuff that's\u00a0 dormant and only being used some of the time,\u00a0\u00a0 but it has more space to look for\u00a0 circuits to do something useful.  \u00a0 Stepping back from the current research questions,\u00a0 I want to understand your modal scenario of what\u00a0\u00a0 happens for the next few years. Towards the\u00a0 beginning of the conversation, we were talking\u00a0\u00a0 about the case in which it progresses really\u00a0 fast, but let's just take the modal scenario.\u00a0 You're unlocking long-horizon RL\u00a0 at some point, but as you said,\u00a0\u00a0 there are potentially other bottlenecks. What's\u00a0 happening? How good are these models? How are\u00a0\u00a0 they being deployed? What other modalities are\u00a0 part of them and at what stage are these being\u00a0\u00a0 unlocked? I want to understand your broader\u00a0 picture of what the next few years look like.  \u00a0 I would expect new modalities to be added\u00a0 over time or pretty soon. I would expect the\u00a0\u00a0 capabilities to generally keep getting better\u00a0 through a combination of pre-training and\u00a0\u00a0 post-training, and that'll open up new use cases. Right now, AI is still not a huge part of the\u00a0\u00a0 economy. There's a pretty small fraction of\u00a0 jobs that it can help with at all. I'd expect\u00a0\u00a0 that to be higher over time, not just from the\u00a0 models improving but also from people figuring\u00a0\u00a0 out how to integrate them into different\u00a0 processes. So even if we just froze the\u00a0\u00a0 models at their current state, you would still\u00a0 see a lot of growth in how they're being used.\u00a0 I would expect AI to be used much more widely\u00a0 and for more technically sophisticated tasks.\u00a0\u00a0 I gave the programming example earlier, doing\u00a0 longer projects, but also helping with various\u00a0\u00a0 kinds of research. I hope that we can use\u00a0 AI to accelerate science in various ways,\u00a0\u00a0 because you can potentially have the models\u00a0 understand all the literature in a given field\u00a0\u00a0 and be able to sift through tons of data. It\u2019s\u00a0 more than a person would have patience to do.\u00a0 I hope the form factor would be such that people\u00a0 are still driving all of this and you have your\u00a0\u00a0 helpful assistants that you can direct and\u00a0 point to lots of different problems that are\u00a0\u00a0 useful to you. Everyone would have all these\u00a0 AIs helping them do more and get more done.  \u00a0 Obviously at some point they're going to be\u00a0 better than everyone at whatever they want to\u00a0\u00a0 do. What would that process look like? Right now,\u00a0 they're clearly only helping you. At some point,\u00a0\u00a0 they\u2019ll be able to just do things for you and\u00a0 maybe run entire firms for you. Is it going to\u00a0\u00a0 be a smooth process? At that point, is the\u00a0 hope that we have systems that are aligned\u00a0\u00a0 with the user enough that they can count on\u00a0 the firm being run in the way they expect.  \u00a0 We might not want to jump to having AIs run whole\u00a0 firms immediately. We might want to have people\u00a0\u00a0 overseeing these important decisions and calling\u00a0 the shots, even if the models are good enough to\u00a0\u00a0 actually run a successful business themselves.\u00a0 To some extent, there might be choices there.\u00a0 I think people will still have different interests\u00a0 and ideas for what kind of interesting pursuits\u00a0\u00a0 they want to direct their AIs at. AI doesn't\u00a0 necessarily have any kind of intrinsic desire,\u00a0\u00a0 unless we put it in the system. So\u00a0 even if AIs become extremely capable,\u00a0\u00a0 I would hope that people are still the\u00a0 drivers of what the AIs end up doing.  \u00a0 I wonder if the economic equilibrium is so\u00a0 far from that, where you have the equivalent\u00a0\u00a0 of Amdahl's law in a firm. The slowest part of the\u00a0 process is the one that's going to bottleneck you.\u00a0 Even if AI makes all the non-human\u00a0 parts of the firm 10X more efficient,\u00a0\u00a0 the firm is still bottlenecked by that step.\u00a0 If one company decides to proceed by keeping\u00a0\u00a0 humans in the loop on all the things that you\u00a0 really want human oversight on, then they'll\u00a0\u00a0 just be outcompeted by other companies. If\u00a0 one country decides to go this route, other\u00a0\u00a0 countries will beat it. I wonder if this is a\u00a0 sustainable plan for keeping humans in the loop.  \u00a0 If we wanted to keep humans in the loop,\u00a0 which seems reasonable, and it turned\u00a0\u00a0 out that firms with any humans in the loop were\u00a0 outcompeted by firms that didn't have any humans,\u00a0\u00a0 then you would obviously need some kind\u00a0 of regulation that disallowed having no\u00a0\u00a0 humans in the loop for running a whole company.   But there are so many companies in any country,\u00a0\u00a0 let alone the world. I wonder if it's\u00a0 better to do the regulation on companies\u00a0\u00a0 and say you've got to keep humans in the\u00a0 loop in important processes, but then you\u00a0\u00a0 have to define what important processes are. You've got to monitor every single company\u00a0\u00a0 and you also have to get collaboration\u00a0 from every single country which has\u00a0\u00a0 firms. If this is a problem, should it be\u00a0 solved before the model is even deployed,\u00a0\u00a0 such that hopefully if you did decide to\u00a0 build a firm and depend on these models,\u00a0\u00a0 it basically does what you want it to do\u00a0 and you don't need a human in the loop?\u00a0 Does that question make sense? I'm\u00a0 just wondering, in this situation,\u00a0\u00a0 how do we actually monitor every single\u00a0 firm to ensure a human is in the loop? And\u00a0\u00a0 what happens if China doesn't decide to do that?   You would either have to have every country agree\u00a0\u00a0 to this regulatory regime, or you would need\u00a0 all of the model infrastructure or the model\u00a0\u00a0 providers to agree to this kind of requirement. It's definitely going to be non-trivial. This\u00a0\u00a0 is looking a ways ahead, so it's a little hard to\u00a0 imagine this world before seeing anything like it.\u00a0 For example, are we actually confident that AI-run\u00a0 companies are better in every way. Do we think\u00a0\u00a0 they're better most of the time, but occasionally\u00a0 they malfunction because AIs are still less\u00a0\u00a0 sample efficient in certain ways? Consider when\u00a0 they have to deal with very wacky situations.\u00a0 AI-run firms might actually have higher tail risk\u00a0 because they're more likely to malfunction in a\u00a0\u00a0 big way. There might be some practical questions\u00a0 like that that would determine how things play\u00a0\u00a0 out. Maybe if you just require people to\u00a0 be accountable for various liabilities,\u00a0\u00a0 this would also change the incentives a bit. Let\u2019s say it turned out that AIs are better\u00a0\u00a0 at running everything and they're also\u00a0 completely benevolent. Let\u2019s say we've\u00a0\u00a0 totally solved alignment, and they're better\u00a0 at being accountable to people than people\u00a0\u00a0 are. Then maybe it's okay having the AIs\u00a0 run the firms. But that's pretty far out.\u00a0 We're more likely to be in a situation where they\u00a0 look better in the short term, but they still have\u00a0\u00a0 some serious problems. It's actually practical\u00a0 considerations that push you more towards having\u00a0\u00a0 humans in the loop, at least for the near future.   So this is a problem we have to deal with today\u00a0\u00a0 with RLHF. You have to aggregate preferences\u00a0 across a lot of different humans. It'll be maybe\u00a0\u00a0 more marked with future, more powerful systems.\u00a0 But when you say we want these eventual AI systems\u00a0\u00a0 that are going to fully replace humans as part of\u00a0 these firms to be aligned, what does that mean?\u00a0 Will it mean that they basically do what the\u00a0 user wants them to do? Does it mean that they\u00a0\u00a0 have to result in some sort of global outcome\u00a0 that we're happy with as the stakeholders in\u00a0\u00a0 OpenAI? Concretely, what would that mean?   If the models are being used for these higher\u00a0\u00a0 stakes use cases, then we would have to\u00a0 think about RLHF in a much different way\u00a0\u00a0 than we are right now.We're not quite ready\u00a0 for that or the current methods might not be\u00a0\u00a0 completely sufficient. We would need to make\u00a0 compromises between the needs of the different\u00a0\u00a0 stakeholders involved. We have this document\u00a0 that we're releasing called the Model Spec.\u00a0\u00a0 It's about how we want our models\u00a0 to behave in the API and in ChatGPT.\u00a0 We try to talk about this issue where there are\u00a0 different stakeholders involved and sometimes\u00a0\u00a0 there are conflicts between what they might\u00a0 want. In our case, we were thinking of the\u00a0\u00a0 stakeholders as the end user (someone sitting in\u00a0 front of ChatGPT or some other app), the developer\u00a0\u00a0 (someone using the API who might be serving other\u00a0 end users with their app), the platform (OpenAI,\u00a0\u00a0 we don't want the models to expose us to legal\u00a0 risk), and the rest of humanity (including\u00a0\u00a0 people not part of the users or customers). Obviously, the user might ask the model to\u00a0\u00a0 do something that we think is actively harmful to\u00a0 other people. We might have to refuse that. By the\u00a0\u00a0 way, this isn't the order of priority necessarily.\u00a0 These are just the four or so classes of\u00a0\u00a0 stakeholder. Actually, you could maybe also say in\u00a0 the future, the model itself. We're not there yet.\u00a0 Anyway, we have these different stakeholders.\u00a0 Sometimes they have conflicting demands. We\u00a0\u00a0 have to make some call on how to resolve those\u00a0 conflicts.It's not always obvious how to do\u00a0\u00a0 that. We had to think through the trade-offs and\u00a0 basically the rough heuristic is that we mostly\u00a0\u00a0 want the models to follow your instructions\u00a0 and be helpful to the user and the developer.\u00a0 But when this impinges on other people's happiness\u00a0 or way of life, this becomes a problem and we have\u00a0\u00a0 to block certain kinds of usage. We mostly want\u00a0 the models to just be an extension of people's\u00a0\u00a0 will and do what they say. We don't want to be\u00a0 too paternalistic. We want to be neutral and\u00a0\u00a0 not impose our opinions on people. We mostly want\u00a0 to let people do what they want with the models.  \u00a0 I got a chance to read the Spec beforehand.\u00a0 This is a question of how well that transfers\u00a0\u00a0 over to how the model itself behaves. I was\u00a0 impressed with how sensible the trade-offs\u00a0\u00a0 were. I believe the actual edge cases were\u00a0 explicitly stated rather than the kinds\u00a0\u00a0 of things where are obvious. In this case,\u00a0 you really are going after the edge cases.  \u00a0 We wanted it to be very actionable so\u00a0 that it wasn't just a bunch of nice\u00a0\u00a0 sounding principles. Each example tells\u00a0 you something about some non-obvious\u00a0\u00a0 situation and reasons through that situation.   I have a couple of questions about the state\u00a0\u00a0 of the research itself. Famously in the social\u00a0 sciences, things are really hard to replicate.\u00a0\u00a0 There\u2019s a question about how much of the\u00a0 science there is real versus these manufactured,\u00a0\u00a0 bespoke sorts of experiments. When you\u00a0 look at the average ML paper, does it\u00a0\u00a0 feel like a really solid piece of literature\u00a0 or does it often feel like the equivalent of\u00a0\u00a0 what p-hacking is in the social sciences?   Everyone has their complaints about the ML\u00a0\u00a0 literature. Overall, I think it's a relatively\u00a0 healthy field especially compared to some others\u00a0\u00a0 like in the social sciences. It's largely grounded\u00a0 in practicality and getting things to work. If you\u00a0\u00a0 publish something that can't be replicated\u00a0 easily, people will just forget about it.\u00a0 It's accepted that often you don't just report\u00a0 someone's number from their paper. You also try\u00a0\u00a0 to reimplement their method and compare it\u00a0 to your method on the same training dataset.\u00a0\u00a0 If you publish methods that are really\u00a0 hard to implement or are really finicky,\u00a0\u00a0 they'll tend to get forgotten. As a result, people actually try\u00a0\u00a0 to open source their work a lot. There are\u00a0 also various unfavorable incentives. People\u00a0\u00a0 are incentivized to make the baseline methods\u00a0 they're comparing to worse. There are other\u00a0\u00a0 mild pathologies, like trying to make your\u00a0 methods seem sophisticated mathematically.\u00a0 But overall, I feel like the field makes progress.\u00a0 I would like to see a little bit more science and\u00a0\u00a0 trying to understand things rather than just hill\u00a0 climbing on benchmarks and trying to propose new\u00a0\u00a0 methods. There's been a decent amount of that\u00a0 recently. We could use more of that. I think\u00a0\u00a0 that's a good thing for academics to work on. On a slightly different note, I'd be really\u00a0\u00a0 excited to see more research on using base models\u00a0 to do simulated social science. These models have\u00a0\u00a0 a probabilistic model of the whole world and\u00a0 you can set up a simulated questionnaire or\u00a0\u00a0 conversation and look at how anything is\u00a0 correlated. Any traits that you might imagine,\u00a0\u00a0 you can see how they might be\u00a0 correlated with other traits.\u00a0 It'd be pretty cool to see if people\u00a0 could replicate some of the more notable\u00a0\u00a0 results in social science, like moral\u00a0 foundations and that sort of thing,\u00a0\u00a0 by just prompting base models in different\u00a0 ways and seeing what's correlated.  \u00a0 What is that Stanford experiment?\u00a0 The Asch conformity test? It'd be\u00a0\u00a0 fun if that replicated with the language\u00a0 models as well. It's very interesting.\u00a0 I want to ask about the rest of the research\u00a0 that happens at big labs. How much of it is\u00a0\u00a0 increasing or decreasing the amount of\u00a0 compute you need to get a certain result\u00a0\u00a0 as an actual compute multiplier versus how\u00a0 much of it is just making the learning more\u00a0\u00a0 stable and building out the infrastructure? The broader question I'm trying to ask is,\u00a0\u00a0 since GPT-4, does it feel like with the same\u00a0 amount of compute, you can train a much better\u00a0\u00a0 model? Or does it feel like you\u2019ve made sure\u00a0 that learning can happen better and in a more\u00a0\u00a0 scalable way with GPT-5, but it's not like we\u00a0 can train GPT-4 with GPT-3.5's budget now?  \u00a0 There's definitely always progress in improving\u00a0 efficiency. Whenever you have a 1D performance\u00a0\u00a0 metric, you're going to find that different\u00a0 improvements can substitute for each other.\u00a0\u00a0 You might find that post-training and pre-training\u00a0 both improve the metrics. They'll have a slightly\u00a0\u00a0 different profile of which metrics they improve. But at the end of the day, if you have a single\u00a0\u00a0 number, they're both going to substitute\u00a0 for each other somewhat. For something\u00a0\u00a0 like a human evaluation, what do humans\u00a0 prefer, we've definitely made a lot of\u00a0\u00a0 progress on both sides, pre-training\u00a0 and post-training, in improving that.  \u00a0 A couple of rapid-fire questions about\u00a0 RLHF. Obviously, RLHF is important to\u00a0\u00a0 make these models useful. So maybe the\u00a0 \"lobotomized\" description is inaccurate.\u00a0 However, there is a sense in which all of these\u00a0 models, once they're put in a chatbot form,\u00a0\u00a0 have a very similar way of speaking. They really\u00a0 want to \u201cdelve\u201d into things. They want to turn\u00a0\u00a0 things into bullet points. They often seem\u00a0 to have this formal and dull way of speaking.\u00a0 There are complaints that they're not as\u00a0 creative. Like we were talking about before,\u00a0\u00a0 they could only do rhyming poetry and not\u00a0 non-rhyming poetry until recently. Is that a\u00a0\u00a0 result of the particular way in which RLHF happens\u00a0 now? If so, is it because of who the raters are?\u00a0\u00a0 Is it because of what the loss function\u00a0 is? Why is this the way all chatbots look?  \u00a0 I would say there's a decent amount of room for\u00a0 variation in exactly how you do the training\u00a0\u00a0 process. We're actively trying to improve\u00a0 this and make the writing more lively and\u00a0\u00a0 fun. We've made some progress like improving\u00a0 the personality of ChatGPT. It is more fun\u00a0\u00a0 and it's better when you're trying to chit\u00a0 chat with it and so forth. It's less robotic.\u00a0 It's an interesting question how some of the\u00a0 ticks came about, like the word \"delve.\" I've\u00a0\u00a0 actually caught myself using that word recently. I\u00a0 don't know if it rubbed off on me from the model.\u00a0 Actually, there might also be some funny effects\u00a0 going on where there's unintentional distillation\u00a0\u00a0 happening between the language model and\u00a0 providers. If you hire someone to go do a labeling\u00a0\u00a0 task, they might just be feeding it into a model.\u00a0 They might be pulling up their favorite chatbot,\u00a0\u00a0 feeding it in, having the model do the task,\u00a0 and then copying and pasting it back. So that\u00a0\u00a0 might account for some of the convergence. Some of the things we're seeing are just what\u00a0\u00a0 people like. People do like bullet points. They\u00a0 like structured responses. People do often like\u00a0\u00a0 the big info dumps that they get from the models. So it's not completely clear how much is just a\u00a0\u00a0 quirk of the particular choices and design of the\u00a0 post-training processes, and how much is actually\u00a0\u00a0 intrinsic to what people actually want.   It does seem persistently more verbose than\u00a0\u00a0 some people want. Maybe it\u2019s just because\u00a0 during the labeling stage, the raters will\u00a0\u00a0 prefer the more verbose answer. I wonder if it's\u00a0 inherent because of how it's pre-trained and the\u00a0\u00a0 stop sequence doesn't come up that often\u00a0 and it really wants to just keep going.  \u00a0 There might be some biases in the labeling\u00a0 that lead to verbosity. There\u2019s the fact\u00a0\u00a0 that we tend to train for one message at a\u00a0 time rather than the full interaction. If you\u00a0\u00a0 only see one message, then something\u00a0 that just has a clarifying question,\u00a0\u00a0 or maybe a short response with an invitation\u00a0 to follow up, is going to look less complete\u00a0\u00a0 than something that covers all possibilities. There's also a question of whether people's\u00a0\u00a0 preferences would change depending on how fast\u00a0 the model is streaming its output. Clearly,\u00a0\u00a0 if you're sitting there waiting for the tokens\u00a0 to come out, you're going to prefer that it gets\u00a0\u00a0 to the point. But if it just gives you a dump\u00a0 of text instantly, maybe you don't actually\u00a0\u00a0 care if there's a bunch of boilerplate or\u00a0 if there's a bunch of stuff you're going\u00a0\u00a0 to skim. You'd rather just have it all there.   The reward model is such an interesting artifact\u00a0\u00a0 because it's the closest thing we have to\u00a0 an aggregation of what people want and what\u00a0\u00a0 preferences they have. I\u2019m thinking\u00a0 about models that are much smarter.\u00a0\u00a0 One hope is that you could just give it a list of\u00a0 things we want that are not trivial and obvious,\u00a0\u00a0 something like the UN Declaration of Human Rights. On the other hand, I think I heard you make the\u00a0\u00a0 point that a lot of our preferences and values\u00a0 are very subtle, so they might be best represented\u00a0\u00a0 through pairwise preferences. When you think\u00a0 of a GPT-6 or GPT-7 level model, are we giving\u00a0\u00a0 it more written instructions or are we still\u00a0 doing these sorts of subliminal preferences?  \u00a0 That's a good question. These preference\u00a0 models do learn a lot of subtleties about\u00a0\u00a0 what people prefer that would be hard\u00a0 to articulate in an instruction manual.\u00a0\u00a0 Obviously, you can write an instruction manual\u00a0 that has lots of examples of comparisons. That's\u00a0\u00a0 what the Model Spec has. It has a lot of examples\u00a0 with some explanations. It's not clear what the\u00a0\u00a0 optimal format is for describing preferences. I would guess that whatever you can get out of\u00a0\u00a0 a big dataset that captures fuzzy preferences,\u00a0 you can distill it down to a shorter document\u00a0\u00a0 that mostly captures the ideas. The bigger models\u00a0 do learn a lot of these concepts automatically of\u00a0\u00a0 what people might find useful and helpful.\u00a0 They'll have some complex moral theories\u00a0\u00a0 that they can latch onto. Of course, there's\u00a0 still a lot of room to latch onto a different\u00a0\u00a0 style or a different morality. So if we were to write a doc,\u00a0\u00a0 if we're going to align these models, what\u00a0 we're doing is latching onto a specific style,\u00a0\u00a0 a specific morality. You still need a decently\u00a0 long document to capture exactly what you want.  \u00a0 How much of a moat is better post-training?\u00a0 Companies distinguish themselves currently by\u00a0\u00a0 how big their model is and so forth. Will it\u00a0 be a big moat for who has figured out all the\u00a0\u00a0 finickiness that you were talking about\u00a0 earlier with regards to all this data?\u2028  \u00a0 There's something of a moat because\u00a0 it's just a very complex operation and\u00a0\u00a0 it takes a lot of skilled people to do\u00a0 it. There's a lot of tacit knowledge and\u00a0\u00a0 organizational knowledge that's required. With post-training, to create a model that\u00a0\u00a0 actually has all the functionality people\u00a0 care about, it\u2019s pretty complicated. It\u00a0\u00a0 requires a pretty complicated effort and\u00a0 accumulation of a lot of R&D. That makes\u00a0\u00a0 it somewhat of a moat. It's not trivial\u00a0 to spin this up immediately. It does seem\u00a0\u00a0 like the same companies that are putting\u00a0 together the most serious pre-training\u00a0\u00a0 efforts are also putting together the\u00a0 most serious post-training efforts.\u00a0 It is somewhat possible to copy or to spin\u00a0 up more of these efforts. There's also one\u00a0\u00a0 force that sort of makes it less of a moat. You\u00a0 can distill the models, or you can take someone\u00a0\u00a0 else's model and clone the outputs. You can use\u00a0 someone else's model as a judge to do comparisons.\u00a0 The more big league people probably aren't\u00a0 doing that because it goes against terms of\u00a0\u00a0 service policies. It would also be a hit to\u00a0 their pride. But I would expect some of the\u00a0\u00a0 smaller players are doing that to get off the\u00a0 ground. That catches you up to a large extent.  \u00a0 I guess it helps clear the moat. What is the\u00a0 median rater like? Where are they based? What are\u00a0\u00a0 their politics? What is their knowledge level?   It varies a lot. We've definitely hired raters\u00a0\u00a0 with different skills for different kinds\u00a0 of tasks or projects. A decent mental model\u00a0\u00a0 is to just look at people who are on Upwork\u00a0 and other platforms like that. Look at who's\u00a0\u00a0 doing odd jobs with remote work. It's a pretty international group.\u00a0\u00a0 There's a decent number of people in the\u00a0 U.S. We hire different groups of people\u00a0\u00a0 for different types of labeling, like whether\u00a0 we're more focused on writing or STEM tasks.\u00a0\u00a0 People doing STEM tasks are more likely to be\u00a0 in India or other middle or lower-middle income\u00a0\u00a0 countries. People doing more English writing\u00a0 and composition tend more to be U.S.-based.\u00a0 There've been times when we needed to hire\u00a0 different experts for some of our campaigns.\u00a0\u00a0 Some of the people are very talented, and we\u00a0 even find that they're at least as good as us,\u00a0\u00a0 the researchers, at doing these tasks and they're\u00a0 much more careful than us. I would say the people\u00a0\u00a0 we have now are quite skilled and conscientious.   With regards to the plateau narrative,\u00a0\u00a0 one of the things I've heard is that a lot of\u00a0 the abilities these models have to help you\u00a0\u00a0 with specific things are related to having very\u00a0 closely matched labels within the supervised\u00a0\u00a0 fine-tuning dataset. Is that true? Can it teach me how to use FFmpeg\u00a0\u00a0 correctly? Is it like there's somebody who\u2019s\u00a0 seeing the inputs, seeing what flags you need\u00a0\u00a0 to add, and some human is figuring that out and\u00a0 matching to that. Do you need to hire all these\u00a0\u00a0 label raters who have domain expertise in all\u00a0 these different domains? If that's the case,\u00a0\u00a0 it seems like it\u2019d be a much bigger slog to get\u00a0 these models to be smarter and smarter over time.  \u00a0 You don't exactly need that. You can get\u00a0 quite a bit out of generalization. The\u00a0\u00a0 base model has already been trained on tons of\u00a0 documentation, code, with shell scripts and so\u00a0\u00a0 forth. It's already seen all the FFmpeg man\u00a0 pages, lots of Bash scripts and everything.\u00a0 Even just giving the base model a good\u00a0 few-shot prompt, you can get it to\u00a0\u00a0 answer queries like this. Just training\u00a0 a preference model for helpfulness will,\u00a0\u00a0 even if you don't train it on any STEM, somewhat\u00a0 generalize to STEM. So not only do you not need\u00a0\u00a0 examples of how to use FFmpeg, you might not\u00a0 even need anything with programming to get some\u00a0\u00a0 reasonable behavior in the programming domain.   Maybe a final question. We've touched on this\u00a0\u00a0 in different ways but let\u2019s put it together. You\u00a0 said you're training on much more multimodal data.\u00a0\u00a0 Presumably, these things understand what screens\u00a0 look like and will be able to interact with them\u00a0\u00a0 in a much more coherent way. Also you're going\u00a0 to do this long-horizon RL, so they'll be able\u00a0\u00a0 to act as agents in the systems and be part of\u00a0 your workflow in a much more integrated way.\u00a0 What do you expect that to look like? What will be\u00a0 the next steps from there? Suppose by the end of\u00a0\u00a0 the year or next year, you have something that's\u00a0 an assistant who can work with you on your screen.\u00a0\u00a0 Does that seem like a sensible thing to\u00a0 expect? Where does it go from there?  \u00a0 I definitely expect things to move in that\u00a0 direction. It's unclear what's going to be\u00a0\u00a0 the best form factor. It could be something that's\u00a0 like a Clippy on your computer helping you or if\u00a0\u00a0 it's more like a helpful colleague in the cloud.\u00a0 We'll see which kinds of form factors work the\u00a0\u00a0 best. I expect people to try all of them out. I expect the mental model of a helpful assistant\u00a0\u00a0 or helpful colleague to become more real. It\u2019ll\u00a0 be something where you can share more of your\u00a0\u00a0 everyday work. Instead of just giving it\u00a0 one-off queries, you would have a whole\u00a0\u00a0 project that you're doing and it knows about\u00a0 everything you've done on that project so far.\u00a0 It can even proactively make suggestions. Maybe\u00a0 you can tell it to remember to ask me about this\u00a0\u00a0 and if I've made any progress on it. Proactivity\u00a0 is one thing that's been missing. I'd love to see\u00a0\u00a0 us moving away from one-off queries, using the\u00a0 model like a search engine, and more towards\u00a0\u00a0 having a whole project that I'm doing in\u00a0 collaboration with the model. Something where it\u00a0\u00a0 knows everything I've done. It's proactively\u00a0 suggesting things for me to try or it's going\u00a0\u00a0 and doing work in the background.   That's really interesting. This is\u00a0\u00a0 the final question. What is your median\u00a0 timeline for when it replaces your job?  \u00a0 Oh, it replaces my job? Maybe five years.   Pretty soon. Interesting. John, this was\u00a0\u00a0 super interesting. Thanks so much for\u00a0 making the time. This seems like one of\u00a0\u00a0 the parts of the AI process that is super\u00a0 important and people don't understand that\u00a0\u00a0 much about. It was super interesting to\u00a0 delve into it and get your thoughts on it.  \u00a0 Thanks for having me on the podcast. It\u00a0 was fun to talk about all this stuff.", "duration": 97}
TASK: you are a novel idea extractor. extract 10 novel ideas about ai OR security OR both from the data below. do not come up with your own ideas but you can elaborate on our ideas in the abstract or summary for each idea. put the ideas and their long abstract in a bulleted list.
